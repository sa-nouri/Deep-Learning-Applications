{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"tcBKvWHD9Mee"},"outputs":[],"source":["#@title Mount drive files\n","from google.colab import drive\n","\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"QZLjJb88DDqx"},"outputs":[],"source":["#@title install libraries\n","\n","!pip install transformers\n","!pip install hazm\n","!pip install -q clean-text[gpl]\n","!pip install stopwords_guilannlp\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z2aGcr539ab1"},"outputs":[],"source":["#@title Load libraries\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","import re\n","import tensorflow as tf\n","import plotly.express as px\n","import plotly.graph_objects as go\n","\n","from tensorflow import keras\n","from cleantext import clean\n","from sklearn.model_selection import train_test_split\n","\n","from hazm import *\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils.np_utils import to_categorical\n","\n","from keras.models import Model, Sequential\n","from keras.layers import Dense, Embedding, Dropout\n","from keras.layers import GlobalMaxPool1D, MaxPooling1D, GlobalMaxPooling1D\n","from keras.layers.convolutional import Conv1D\n","from keras.metrics import categorical_accuracy, categ\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vMP0u2em9cGp"},"outputs":[],"source":["#@title Define data cleaning/processing functions \n","\n","def cleanhtml(raw_html):\n","    cleanr = re.compile('<.*?>')\n","    cleantext = re.sub(cleanr, '', raw_html)\n","    return cleantext\n","\n","\n","def cleaning(text):\n","    text = text.strip()\n","    \n","    # regular cleaning\n","    text = clean(text,\n","        fix_unicode=True,\n","        to_ascii=False,\n","        lower=True,\n","        no_line_breaks=True,\n","        no_urls=True,\n","        no_emails=True,\n","        no_phone_numbers=True,\n","        no_numbers=False,\n","        no_digits=False,\n","        no_currency_symbols=True,\n","        no_punct=False,\n","        replace_with_url=\"\",\n","        replace_with_email=\"\",\n","        replace_with_phone_number=\"\",\n","        replace_with_number=\"\",\n","        replace_with_digit=\"0\",\n","        replace_with_currency_symbol=\"\",\n","    )\n","\n","    # cleaning htmls\n","    text = cleanhtml(text)\n","    \n","    # normalizing\n","    normalizer = Normalizer()\n","    text = normalizer.normalize(text)\n","    \n","    # removing wierd patterns\n","    wierd_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","        u\"\\U0001f926-\\U0001f937\"\n","        u'\\U00010000-\\U0010ffff'\n","        u\"\\u200d\"\n","        u\"\\u2640-\\u2642\"\n","        u\"\\u2600-\\u2B55\"\n","        u\"\\u23cf\"\n","        u\"\\u23e9\"\n","        u\"\\u231a\"\n","        u\"\\u3030\"\n","        u\"\\ufe0f\"\n","        u\"\\u2069\"\n","        u\"\\u2066\"\n","        # u\"\\u200c\"\n","        u\"\\u2068\"\n","        u\"\\u2067\"\n","        \"]+\", flags=re.UNICODE)\n","    \n","    text = wierd_pattern.sub(r'', text)\n","    \n","    # removing extra spaces, hashtags\n","    text = re.sub(\"#\", \"\", text)\n","    text = re.sub(\"\\s+\", \" \", text)\n","    \n","    return text\n","\n","\n","def data_gl_than(data, less_than=100.0, greater_than=0.0,\n","                 col='description_len'):\n","    data_length = data[col].values\n","\n","    data_glt = sum([1 for length in data_length if greater_than < length <= less_than])\n","\n","    data_glt_rate = (data_glt / len(data_length)) * 100\n","\n","    print(f'''Texts with word length of greater than {greater_than} and\n","           less than {less_than} includes {data_glt_rate:.2f}% of the whole!''')\n","\n","\n","def process_data(data, data_type='train',\n","                 remove_description=False) -> pd.DataFrame:\n","\n","    if data_type == 'train':\n","      data['label'] = data['label'].astype(str)\n","\n","      data = data.dropna(subset=['label'])\n","      data = data.dropna(subset=['description_fa'])\n","      data = data.reset_index(drop=True)\n","\n","    data['description_len'] = data['description_fa'].apply(lambda t: len(word_tokenize(t)))\n","    min_max_len = data[\"description_len\"].min(), data[\"description_len\"].max()\n","    data_gl_than(data, maxlim, minlim)\n","\n","    if remove_description:\n","      # remove comments with the length of fewer than three words\n","      data['description_len'] = data['description_len'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else None)\n","      data = data.dropna(subset=['description_len'])\n","      data = data.reset_index(drop=True)\n","\n","    data['cleaned_description'] = data['description_fa'].apply(cleaning)\n","\n","    # calculate the length of comments based on their words\n","    data['cleaned_desc_len'] = data['cleaned_description'].apply(lambda t: len(word_tokenize(t)))\n","\n","    if remove_description:\n","      # remove comments with the length of fewer than three words\n","      data['cleaned_desc_len'] = data['cleaned_desc_len'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else len_t)\n","      data = data.dropna(subset=['cleaned_desc_len'])\n","      data = data.reset_index(drop=True)\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AbGvs2k29JCC"},"outputs":[],"source":["#@title data processing\n","\n","TEST_SIZE: int = 0.1\n","minlim, maxlim = 3, 320\n","\n","train_data = pd.read_csv('/content/drive/MyDrive/datacamp/classification/train_set.zip')\n","test_data = pd.read_csv('/content/drive/MyDrive/datacamp/classification/test_set.zip')\n","\n","train = process_data(train_data)\n","test = process_data(test_data, data_type='test')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"nUT9LNX6XHEI"},"outputs":[],"source":["#@title plot dataset\n","fig = go.Figure()\n","\n","fig.add_trace(go.Histogram(\n","    x=train['description_len']\n","))\n","\n","fig.update_layout(\n","    title_text='Distribution of word counts within description',\n","    xaxis_title_text='Word Count',\n","    yaxis_title_text='Frequency',\n","    bargap=0.2,\n","    bargroupgap=0.2)\n","\n","fig.show()\n","\n","\n","fig = go.Figure()\n","\n","groupby_rate = train.groupby('label')['label'].count()\n","\n","fig.add_trace(go.Bar(\n","    x=list(sorted(groupby_rate.index)),\n","    y=groupby_rate.tolist(),\n","    text=groupby_rate.tolist(),\n","    textposition='auto'\n","))\n","\n","fig.update_layout(\n","    title_text='Distribution of label within description',\n","    xaxis_title_text='Label',\n","    yaxis_title_text='Frequency',\n","    bargap=0.2,\n","    bargroupgap=0.2)\n","\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Go_SuQvQCQj0"},"outputs":[],"source":["#@title clean documnet and tokenization\n","\n","puncs = ['ØŒ', '.', ',', ':', ';', '\"']\n","normalizer = Normalizer()\n","lemmatizer = Lemmatizer()\n","\n","# turn a doc into clean tokens\n","def clean_doc(doc):\n","    doc = normalizer.normalize(doc) # Normalize document using Hazm Normalizer\n","    tokenized = word_tokenize(doc)  # Tokenize text\n","    tokens = []\n","    for t in tokenized:\n","      temp = t\n","      for p in puncs:\n","        temp = temp.replace(p, '')\n","      tokens.append(temp)\n","    # tokens = [w for w in tokens if not w in stop_set]    # Remove stop words\n","    tokens = [w for w in tokens if not len(w) <= 1]\n","    tokens = [w for w in tokens if not w.isdigit()]\n","    tokens = [lemmatizer.lemmatize(w) for w in tokens] # Lemmatize sentence words using Hazm Lemmatizer\n","    tokens = ' '.join(tokens)\n","    return tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-NMCLkp8j9N"},"outputs":[],"source":["#@title prepare data\n","\n","NUM_WORDS:int = 2000\n","NUM_CLASSES: int = 10\n","tokenizer = Tokenizer(num_words=NUM_WORDS)\n","\n","x_train = np.array(train['cleaned_description'])\n","y_train = np.array(train['label'])\n","\n","x_test = np.array(test['cleaned_description'])\n","\n","# Apply preprocessing step to training data and test data\n","train_docs = np.empty_like(x_train)\n","for index, document in enumerate(x_train):\n","  train_docs[index] = clean_doc(document)\n","\n","test_docs = np.empty_like(x_test)\n","for index, document in enumerate(x_test):\n","  test_docs[index] = clean_doc(document)\n","\n","tokenizer.fit_on_texts(train_docs)\n","max_length = max([len(s.split()) for s in train_docs])\n","\n","# Embed and Pad embede training sequences\n","encoded_docs = tokenizer.texts_to_sequences(train_docs)\n","encoded_docs_test = tokenizer.texts_to_sequences(test_docs)\n","\n","x_train_padded = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","x_test_padded = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')\n","vocab_size = len(tokenizer.word_index)\n","\n","# Prepare input data to the model\n","categorical_y_train = to_categorical(y_train, NUM_CLASSES)\n","in_train, in_test, out_label, out_test = train_test_split(x_train_padded,\n","                                                          categorical_y_train,\n","                                                          test_size=TEST_SIZE,\n","                                                          random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iky57pvDDhfR"},"outputs":[],"source":["#@title Build model\n","\n","from keras.optimizers import schedules, Adam\n","from keras.regularizers import l2\n","\n","model_cnn = Sequential()\n","model_cnn.add(Embedding(vocab_size, 400, input_length=max_length))\n","\n","model_cnn.add(Conv1D(filters=64, kernel_size=4, activation='relu',\n","                     padding='same', kernel_regularizer=l2(0.1)))\n","model_cnn.add(MaxPooling1D(pool_size=2))\n","\n","model_cnn.add(Conv1D(filters=64, kernel_size=8, activation='relu',\n","                     padding='same', kernel_regularizer=l2(0.1) ))\n","model_cnn.add(MaxPooling1D(pool_size=2))\n","\n","model_cnn.add(Conv1D(filters=64, kernel_size=16, activation='relu', \n","                     padding='same', kernel_regularizer=l2(0.1)))\n","model_cnn.add(GlobalMaxPooling1D())\n","\n","model_cnn.add(Dropout(0.5))\n","model_cnn.add(Dense(300, activation=\"sigmoid\"))\n","\n","# model_cnn.add(Dropout(0.4))\n","# model_cnn.add(Dense(100, activation=\"relu\"))\n","\n","model_cnn.add(Dense(NUM_CLASSES, activation='softmax'))\n","\n","# lr_schedule = schedules.ExponentialDecay(\n","#                                         initial_learning_rate=4e-3,\n","#                                         decay_steps=2,\n","#                                         decay_rate=0.99999\n","#                                         )\n","opt = Adam(learning_rate=2e-3,\n","                            beta_1=0.9,\n","                            beta_2=0.999,\n","                            epsilon=1e-07,\n","                            amsgrad=False\n","                        )\n","\n","model_cnn.compile(loss='categorical_crossentropy',\n","              optimizer=opt,\n","              metrics=[categorical_accuracy])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LzZ9ssyoFy4t"},"outputs":[],"source":["#@title Training\n","\n","BATCH_SIZE =  64#@param {type:\"number\"}\n","EPOCHS =   8#@param {type:\"number\"}\n","\n","# Train model\n","history_cnn = model_cnn.fit(in_train, out_label,\n","                         batch_size=BATCH_SIZE, epochs=EPOCHS,\n","                         validation_split=0.1,\n","                         shuffle=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vonza179Iq_Q"},"outputs":[],"source":["model_cnn.save('my_model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"m28qZh8Q_PYs"},"outputs":[],"source":["#@title Model Evaluation\n","\n","BATCH_SIZE = 64 #@param {type:\"number\"}\n","\n","model_cnn.evaluate(\n","    x=in_test,\n","    y=out_test,\n","    batch_size=BATCH_SIZE\n",")\n","\n","from sklearn.metrics import balanced_accuracy_score\n","\n","# predicted_val = np.argmax(model_cnn.predict(in_test), axis=-1)\n","# balanced_accuracy_score(out_test, predicted_val)\n","out_test"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"cnWlGzw5AjjD"},"outputs":[],"source":["#@title Predict test labels\n","\n","BATCH_SIZE = 128 #@param {type:\"number\"}\n","\n","model_cnn.predict(\n","    x_test_padded,\n","    batch_size=BATCH_SIZE\n",")\n","\n","test_labels = np.argmax(model_cnn.predict(x_test_padded), axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGpZB_eWJYIe"},"outputs":[],"source":["# test_labels = model_cnn.predict_classes(x_test_padded)\n","test_y = pd.DataFrame(test_labels, columns=['label'])\n","test_y['app_id'] = test_data['app_id']\n","test_y = test_y[['app_id', 'label']]\n","test_y.to_csv('prediction.csv', index=False)\n","test_y.head(10)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNgfFNch7ZZeT2g4ZLIWEtS","collapsed_sections":[],"name":"games_classification.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.1 (main, Dec 23 2022, 09:25:23) [Clang 14.0.0 (clang-1400.0.29.202)]"},"vscode":{"interpreter":{"hash":"5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"}}},"nbformat":4,"nbformat_minor":0}

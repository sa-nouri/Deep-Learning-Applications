{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21582,"status":"ok","timestamp":1638169384594,"user":{"displayName":"Salar Nouri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhWx_6jpGSB71vN_x6U0X8AoHQmv44rArelw2aojw=s64","userId":"04074158743789529609"},"user_tz":-210},"id":"1jP_-bR1tc-2","outputId":"a5d559a7-55cf-4031-f990-fc7ae228a685"},"outputs":[],"source":["#@title Mount drive files\n","from google.colab import drive\n","\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"njaCzngpx1Ph"},"outputs":[],"source":["#@title Install necassary libraries\n","\n","!pip install fastparquet\n","!pip install -q clean-text[gpl]\n","!pip install hazm\n","# !pip install transformers\n","# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.bin.gz\n","# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fa.300.vec.gz\n","!cp \"/content/drive/MyDrive/Projects/datacamp/Text/cc.fa.300.vec.gz\" -r \"/content\"\n","!gunzip /content/cc.fa.300.vec.gz\n","# !rm \"/content/cc.fa.300.vec.gz\"\n","!pip install fasttext"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"gjExWqLt0B5K"},"outputs":[],"source":["#@title Load libraries\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","import re\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","# import fasttext\n","import keras.backend as K\n","\n","from fastparquet import ParquetFile\n","from json import loads\n","from cleantext import clean\n","from hazm import Normalizer, Lemmatizer, word_tokenize\n","from sklearn.model_selection import train_test_split\n","from keras.utils.np_utils import to_categorical\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","# from transformers import BertTokenizer, BasicTokenizer, TFBertModel\n","# from transformers import TFBertPreTrainedModel, TFBertForSequenceClassification\n","# from transformers import glue_convert_examples_to_features, InputExample\n","\n","from keras.models import Model, Sequential\n","from keras.models import load_model\n","from keras.layers import Dense, Embedding, Dropout, BatchNormalization, Flatten\n","from keras.layers import GlobalMaxPool1D, MaxPooling1D, GlobalMaxPooling1D\n","from keras.layers import Conv1D, LSTM, GRU, Bidirectional, SimpleRNN\n","from keras.layers import multiply, Input, Concatenate\n","from tensorflow.keras.optimizers import Adam, schedules\n","from keras.regularizers import l2\n","from keras import losses, metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"jPRZNNxhCiqr"},"outputs":[],"source":["#@title Load Dataset\n","\n","test_dir = '/content/drive/MyDrive/Projects/datacamp/Text/datasets/future_test.parquet'\n","reject_dir = '/content/drive/MyDrive/Projects/datacamp/Text/datasets/reject_reasons_info.csv'\n","train_dir = '/content/drive/MyDrive/Projects/datacamp/Text/datasets/train.parquet'\n","\n","df_rejects = pd.read_csv(reject_dir)\n","pf_train = ParquetFile(train_dir)\n","pf_test = ParquetFile(test_dir)\n","\n","df_train = pf_train.to_pandas()\n","df_test = pf_test.to_pandas()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"VTdJrO1A-_Rs"},"outputs":[],"source":["#@title Preview Data\n","\n","print('Train Data ============================================================')\n","df_train.info()\n","print()\n","print('Test Data =============================================================')\n","df_test.info()\n","\n","display(df_train.head(10))\n","print()\n","display(df_rejects.head(10))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"vI4BLHvQ56JS"},"outputs":[],"source":["#@title Modify dataset\n","\n","def get_features(row, feature):\n","    try:\n","        return row[feature]\n","    except:\n","        return np.nan\n","\n","print(df_train.review_label.unique())\n","print(df_test.review_label.unique())\n","\n","# df_train['label'] = df_train['reject_reason_id'].apply(\n","                            # lambda rec: 0 if rec == 0 else 1)\n","\n","df_train['label'] = df_train['review_label'].apply(\n","                            lambda rec: 1 if rec == 'reject' else 0)\n","\n","df_test['label'] = df_train['review_label'].apply(\n","                            lambda rec: 1 if rec == 'reject' else 0)\n","\n","df_train['interview_post'] = df_train['interview_post'].apply(\n","                                            lambda rec: loads(rec))\n","\n","df_test['interview_post'] = df_test['interview_post'].apply(\n","                                            lambda rec: loads(rec))\n","\n","# ['category', 'description', 'elevator', 'floor', 'location',\n","#  'other_options_and_attributes', 'parking', 'price', 'rooms',\n","#  'size', 'title', 'user_type', 'warehouse', 'year']\n","\n","total_features = df_train.interview_post.iloc[0].keys()\n","price_features = ['mode', 'value']\n","\n","for feature in total_features:\n","    df_train[feature] = df_train['interview_post'].apply(\n","                                        lambda rec: get_features(rec, feature))\n","    df_test[feature] = df_test['interview_post'].apply(\n","                                        lambda rec: get_features(rec, feature))\n","\n","for item in price_features:\n","    df_train['price_' + item] = df_train['price'].apply(\n","                                        lambda rec: get_features(rec, item))\n","    df_test['price_' + item] = df_test['price'].apply(\n","                                        lambda rec: get_features(rec, item))\n","\n","df_train.drop(columns=['interview_post', 'price'], inplace=True)\n","df_test.drop(columns=['interview_post', 'price'], inplace=True)\n","display(df_train.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"28UNylZdE3WJ"},"outputs":[],"source":["#@title Plot distriubtion of rejects\n","\n","bins = (df_train.reject_reason_id.unique())\n","print(f\"Reject reasons are {bins}\")\n","print(f\"Bins value is {len(bins)}!\")\n","print()\n","df_train[['reject_reason_id', 'label']].hist(bins=len(bins));"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"2HGDZy1yqyzO"},"outputs":[],"source":["#@title Cleaning descriptions\n","\n","def cleanhtml(raw_html):\n","    cleanr = re.compile('<.*?>')\n","    cleantext = re.sub(cleanr, '', raw_html)\n","    return cleantext\n","\n","\n","def cleaning(text):\n","    text = text.strip()\n","    \n","    # regular cleaning\n","    text = clean(text,\n","        fix_unicode=True,\n","        to_ascii=False,\n","        lower=True,\n","        no_line_breaks=True,\n","        no_urls=True,\n","        no_emails=True,\n","        no_phone_numbers=True,\n","        no_numbers=False,\n","        no_digits=False,\n","        no_currency_symbols=True,\n","        no_punct=False,\n","        replace_with_url=\"\",\n","        replace_with_email=\"\",\n","        replace_with_phone_number=\"\",\n","        replace_with_number=\"\",\n","        replace_with_digit=\"0\",\n","        replace_with_currency_symbol=\"\",\n","    )\n","\n","    # cleaning htmls\n","    text = cleanhtml(text)\n","    \n","    # normalizing\n","    normalizer = Normalizer()\n","    text = normalizer.normalize(text)\n","    \n","    # removing wierd patterns\n","    wierd_pattern = re.compile(\"[\"\n","        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","        u\"\\U0001f926-\\U0001f937\"\n","        u'\\U00010000-\\U0010ffff'\n","        u\"\\u200d\"\n","        u\"\\u2640-\\u2642\"\n","        u\"\\u2600-\\u2B55\"\n","        u\"\\u23cf\"\n","        u\"\\u23e9\"\n","        u\"\\u231a\"\n","        u\"\\u3030\"\n","        u\"\\ufe0f\"\n","        u\"\\u2069\"\n","        u\"\\u2066\"\n","        # u\"\\u200c\"\n","        u\"\\u2068\"\n","        u\"\\u2067\"\n","        \"]+\", flags=re.UNICODE)\n","    \n","    text = wierd_pattern.sub(r'', text)\n","    \n","    # removing extra spaces, hashtags\n","    text = re.sub(\"#\", \"\", text)\n","    text = re.sub(\"\\s+\", \" \", text)\n","    \n","    return text\n","\n","df_train['description'] = df_train['description'].apply(cleaning)\n","df_test['description'] = df_test['description'].astype(str)\n","df_test['description'] = df_test['description'].apply(cleaning)\n","\n","df_train['title'] = df_train['title'].apply(cleaning)\n","df_test['title'] = df_test['title'].apply(cleaning)\n","\n","df_train['city'] = df_train['location'].apply(\n","                                        lambda rec: get_features(rec, 'city'))\n","df_test['city'] = df_test['location'].apply(\n","                                        lambda rec: get_features(rec, 'city'))\n","\n","df_train['context'] = df_train['description'] + df_train['title'] + \\\n","                    df_train['city'].astype(str) + df_train['price_value'].astype(str)\n","\n","df_test['context'] = df_test['description'] + df_test['title'] + \\\n","                    df_test['city'].astype(str) + df_test['price_value'].astype(str)\n","\n","df_train['price_value'].fillna(-1, inplace=True)\n","df_test['price_value'].fillna(-1, inplace=True)\n","\n","df_train['label'] = df_train['label'].astype(str)\n","df_test['label'] = df_test['label'].astype(str)\n","train_data, val_data = train_test_split(df_train, test_size=0.2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"puTOTV4lpIPT"},"outputs":[],"source":["#@title Bert preprocessing\n","\n","def convert_data_into_input_example(data):\n","    input_examples = []\n","    for i in range(len(data)):\n","        example = InputExample(\n","            guid= None,\n","            text_a= data.iloc[i]['description'],\n","            text_b= None,\n","            label= data.iloc[i]['label']\n","        )\n","        input_examples.append(example)\n","    return input_examples\n","\n","\n","train_examples = convert_data_into_input_example(train_data)\n","val_examples = convert_data_into_input_example(val_data)\n","test_examples = convert_data_into_input_example(df_test)\n","\n","# tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',\n","                                        # do_lower_case=False)\n","\n","label_list = ['0', '1']\n","bert_train_dataset = glue_convert_examples_to_features(examples=train_examples,\n","                                                       tokenizer=tokenizer,\n","                                                       max_length=128,\n","                                                       task='mrpc',\n","                                                       label_list=label_list)\n","\n","bert_val_dataset = glue_convert_examples_to_features(examples=val_examples,\n","                                                     tokenizer=tokenizer,\n","                                                     max_length=128,\n","                                                     task='mrpc',\n","                                                     label_list=label_list)\n","\n","bert_test_dataset = glue_convert_examples_to_features(examples=test_examples,\n","                                                     tokenizer=tokenizer,\n","                                                     max_length=128,\n","                                                     tglue_convert_examples_to_featuresask='mrpc',\n","                                                     label_list=label_list)\n","\n","# model = TFBertForSequenceClassification.from_pretrained(\n","                                                # 'bert-base-multilingual-cased')\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,\n","                                     epsilon=1e-08,\n","                                     clipnorm=1.0)\n","loss = losses.SparseCategoricalCrossentropy(from_logits=True)\n","metric = metrics.SparseCategoricalAccuracy('accuracy')\n","model.compile(loss=loss,\n","              optimizer=optimizer,\n","              metrics=[metric])"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"eVk8sjQ4menk"},"outputs":[],"source":["#@title Bert Training\n","\n","EPOCHS = 4 #@param {type: 'number'}\n","\n","def modify_ds(bdset):   \n","    input_ids, attention_mask, token_type_ids, label = [], [], [], []\n","    for in_ex in bdset:\n","        input_ids.append(in_ex.input_ids)\n","        attention_mask.append(in_ex.attention_mask)\n","        token_type_ids.append(in_ex.token_type_ids)\n","        label.append(in_ex.label)\n","\n","    input_ids = np.vstack(input_ids)\n","    attention_mask = np.vstack(attention_mask)\n","    token_type_ids = np.vstack(token_type_ids)\n","    label = np.vstack(label)\n","    return ([input_ids, attention_mask, token_type_ids], label)\n","\n","def example_to_features(input_ids, attention_masks, token_type_ids, y):\n","    return {\"input_ids\": input_ids,\n","            \"attention_mask\": attention_masks,\n","            \"token_type_ids\": token_type_ids},y\n","\n","\n","x_train, y_train = modify_ds(bert_train_dataset)\n","x_val, y_val = modify_ds(bert_val_dataset)\n","x_test, y_test = modify_ds(bert_test_dataset)\n","\n","train_ds = tf.data.Dataset.from_tensor_slices((x_train[0], x_train[1],\n","                                               x_train[2], y_train)).map(example_to_features).shuffle(100).batch(32)\n","val_ds   = tf.data.Dataset.from_tensor_slices((x_val[0], x_val[1], \n","                                               x_val[2], y_val)).map(example_to_features).batch(64)\n","test_ds   = tf.data.Dataset.from_tensor_slices((x_test[0], x_test[1], \n","                                               x_test[2], y_test)).map(example_to_features).batch(64)\n","\n","\n","print('format of model input examples: '.format(train_ds.take(1)))\n","\n","history = model.fit(train_ds,\n","                    validation_data=val_ds,\n","                    epochs=EPOCHS)\n","\n","model.save('/content/drive/MyDrive/Projects/datacamp/Text/bert_model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rgCdOON4Guar"},"outputs":[],"source":["#@title Bert Test\n","\n","model = load_model('/content/drive/MyDrive/Projects/datacamp/Text/bert_model.h5')\n","test_result = model.evaluate(test_ds)\n","predictions = model.predict(test_ds)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"4ALACwMqm0xj"},"outputs":[],"source":["#@title Tokenization and cleaning\n","\n","puncs = ['،', '.', ',', ':', ';', '\"']\n","normalizer = Normalizer()\n","lemmatizer = Lemmatizer()\n","\n","# turn a doc into clean tokens\n","def clean_doc(doc):\n","    doc = normalizer.normalize(doc) # Normalize document using Hazm Normalizer\n","    tokenized = word_tokenize(doc)  # Tokenize text\n","    tokens = []\n","    for t in tokenized:\n","      temp = t\n","      for p in puncs:\n","        temp = temp.replace(p, '')\n","      tokens.append(temp)\n","    # tokens = [w for w in tokens if not w in stop_set]    # Remove stop words\n","    tokens = [w for w in tokens if not len(w) <= 1]\n","    tokens = [w for w in tokens if not w.isdigit()]\n","    tokens = [lemmatizer.lemmatize(w) for w in tokens] # Lemmatize sentence words using Hazm Lemmatizer\n","    tokens = ' '.join(tokens)\n","    return tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"SHXtZOvuJ_-N"},"outputs":[],"source":["#@title Prepare data\n","\n","NUM_WORDS = 2000\n","NUM_CLASSES = 2\n","TEST_SIZE = 0.2\n","\n","tokenizer = Tokenizer(num_words=NUM_WORDS)\n","\n","x_train = np.array(df_train['context'])\n","y_train = np.array(df_train['label'])\n","\n","x_test = np.array(df_test['context'])\n","y_test = np.array(df_test['label'])\n","\n","# Apply preprocessing step to training data and test data\n","train_docs = np.empty_like(x_train)\n","for index, document in enumerate(x_train):\n","  train_docs[index] = clean_doc(document)\n","\n","test_docs = np.empty_like(x_test)\n","for index, document in enumerate(x_test):\n","  test_docs[index] = clean_doc(document)\n","\n","tokenizer.fit_on_texts(train_docs)\n","max_length = max([len(s.split()) for s in train_docs])\n","\n","# Embed and Pad embeded training sequences\n","encoded_train = tokenizer.texts_to_sequences(train_docs)\n","encoded_test = tokenizer.texts_to_sequences(test_docs)\n","\n","x_train_padded = pad_sequences(encoded_train, maxlen=max_length,\n","                               padding='post')\n","x_test_padded = pad_sequences(encoded_test, maxlen=max_length,\n","                              padding='post')\n","vocab_size = len(tokenizer.word_index)\n","\n","categorical_y_train = to_categorical(y_train, NUM_CLASSES)\n","categorical_y_test = to_categorical(y_test, NUM_CLASSES)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"DIDikcv-O1Nc"},"outputs":[],"source":["#@title Build CNN model \n","\n","Metrics = [\n","    metrics.Precision(name=\"precision\"),\n","    metrics.Accuracy(name='accuracy'),\n","    metrics.Recall(name=\"recall\"),\n","    metrics.AUC(name='auc'),\n","    metrics.AUC(name='prc', curve='PR')\n","]\n","\n","model_cnn = Sequential()\n","model_cnn.add(Embedding(vocab_size, 400, input_length=max_length))\n","\n","model_cnn.add(Conv1D(filters=64, kernel_size=4, activation='relu', \n","                     padding='same', kernel_regularizer=l2(0.1)))\n","model_cnn.add(BatchNormalization())\n","model_cnn.add(MaxPooling1D(pool_size=2))\n","model_cnn.add(Dropout(0.3))\n","\n","model_cnn.add(Conv1D(filters=64, kernel_size=8, activation='relu',\n","                     padding='same', kernel_regularizer=l2(0.1) ))\n","model_cnn.add(MaxPooling1D(pool_size=2))\n","model_cnn.add(Dropout(0.3))\n","\n","model_cnn.add(Conv1D(filters=64, kernel_size=16, activation='relu', \n","                     padding='same', kernel_regularizer=l2(0.1)))\n","model_cnn.add(GlobalMaxPooling1D())\n","model_cnn.add(Dropout(0.5))\n","\n","model_cnn.add(Dense(300, activation=\"relu\"))\n","model_cnn.add(Dropout(0.5))\n","\n","# model_cnn.add(Dense(100, activation=\"relu\"))\n","# model_cnn.add(Dropout(0.5))\n","\n","# model_cnn.add(Dense(100, activation=\"relu\"))\n","# model_cnn.add(Dropout(0.5))\n","\n","model_cnn.add(Dense(NUM_CLASSES, activation='softmax'))\n","\n","# lr_schedule = schedules.ExponentialDecay(\n","#                                         initial_learning_rate=4e-3,\n","#                                         decay_steps=2,\n","#                                         decay_rate=0.99999\n","#                                         )\n","\n","optimizer = Adam(learning_rate=5e-4, beta_1=0.9, beta_2=0.999, \n","                epsilon=1e-07, amsgrad=False)\n","\n","model_cnn.compile(loss='categorical_crossentropy',\n","                  optimizer=optimizer,\n","                  metrics=metrics.categorical_accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"-YE92wAQUPBC"},"outputs":[],"source":["#@title Training CNN\n","\n","BATCH_SIZE = 64 #@param {type:\"number\"}\n","EPOCHS = 10 #@param {type:\"number\"}\n","\n","# Train model\n","history_cnn = model_cnn.fit(x_train_padded,\n","                            categorical_y_train,\n","                            batch_size=BATCH_SIZE,\n","                            epochs=EPOCHS,\n","                            validation_split=0.2,\n","                            shuffle=True)\n","\n","model_cnn.save('/content/drive/MyDrive/Projects/datacamp/Text/cnn_model.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WQ85sj9_NAWD"},"outputs":[],"source":["#@title CNN Test\n","\n","threshold = 0.1\n","model = load_model('/content/drive/MyDrive/Projects/datacamp/Text/cnn_model.h5')\n","test_result = model_cnn.evaluate(x_test_padded, categorical_y_test)\n","predictions = model_cnn.predict(x_test_padded)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4kbUqb4R2yA"},"outputs":[],"source":["#@title Get dictionary\n","\n","def get_dict(df):\n","    wordDict = {}\n","    for idx, row in enumerate(df.context):\n","        price = row.split()[-1]\n","        row = row + price\n","        row = re.split(r'([a-zA-Z]+)', row)\n","        row = \" \".join(str(item) for item in row)\n","        words = row.split()\n","        for wrd in words:\n","            if wrd in wordDict:\n","                wordDict[wrd] += 1\n","            else:\n","                wordDict[wrd] = 1\n","    return wordDict\n","\n","train_wordDict = get_dict(df_train)\n","test_wordDict = get_dict(df_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"IZaiOmgsZ61F"},"outputs":[],"source":["#@title Tockenize data\n","\n","MAX_NB_WORDS = 55000\n","MAX_SEQUENCE_LENGTH = 500\n","\n","content_train = df_train['context']\n","content_test = df_test['context']\n","\n","y_train = np.array(df_train['label'])\n","y_test = np.array(df_test['label'])\n","\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n","tokenizer.fit_on_texts(content_train)\n","word_index = tokenizer.word_index\n","\n","train_sequences = tokenizer.texts_to_sequences(content_train)\n","test_sequences = tokenizer.texts_to_sequences(content_test)\n","\n","train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n","\n","# x_train, x_val, y_train, y_val  = train_test_split(train_data, y_train,\n","                                                #    test_size=0.2, random_state=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true},"id":"FapM7ZV9_KNk"},"outputs":[],"source":["#@title Get Embeddings by fastText\n","\n","fastTextDir = '/content/'\n","fastText_path = os.path.join(fastTextDir, 'cc.fa.300.vec')\n","\n","def get_embedding(wordDict):\n","    embeddings_index = {}\n","    with open(fastText_path, encoding='utf8') as infile:\n","        for line in infile:\n","            values = line.split()\n","            word = values[0]\n","            try:\n","                coefs = np.asarray(values[1:], dtype='float32')\n","            except:\n","                print(\"Warnning\"+str(values)+\" in\" + str(line))\n","            if word in wordDict:\n","                embeddings_index[word] = coefs\n","    return embeddings_index\n","\n","train_embeddings = get_embedding(train_wordDict)\n","test_embeddings = get_embedding(test_wordDict)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true},"id":"YHBcMRtYViGt"},"outputs":[],"source":["#@title Get Embedding matrix\n","\n","EMBEDDING_DIM = 300\n","embeddings_index = train_embeddings\n","embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"tzOs93s0dI_i"},"outputs":[],"source":["#@title RNN model\n","\n","from tensorflow import expand_dims\n","\n","nClasses = 2\n","type = 1\n","\n","model = Sequential()\n","input1 = Input((MAX_SEQUENCE_LENGTH - 1, ), name='context')\n","layerM1Embedding = Embedding(len(word_index) + 1,\n","                            EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            input_length=MAX_SEQUENCE_LENGTH,\n","                            trainable=True)(input1)\n","\n","input2 = Input((1,), name='price')\n","if type == 1:\n","    layer = GRU(100, dropout=0.2, recurrent_dropout=0.2)(layerM1Embedding)\n","    layer = expand_dims(layer, axis=-1)\n","    layer = Conv1D(filters=64, kernel_size=2, padding='same',\n","                   activation='relu')(layer)\n","    layer = MaxPooling1D(pool_size=2)(layer)\n","    layer = LSTM(200, dropout=0.2, recurrent_dropout=0.2)(layer)\n","elif type == 2:\n","    layerM1 = GRU(100, dropout=0.2, recurrent_dropout=0.2)(layerM1Embedding)\n","    layerM1 = Dense(nClasses, activation='softmax')(layerM1)\n","    layerM2 = Dense(nClasses, activation='softmax')(input2)\n","    layer = Concatenate()([layerM1, layerM2])     \n","\n","out = Dense(nClasses, activation='softmax')(layer)\n","model = Model(inputs=[input1, input2], outputs=out)\n","\n","optimizer = Adam(learning_rate=5e-4, beta_1=0.9, beta_2=0.999, \n","                epsilon=1e-07, amsgrad=False)\n","\n","model.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n","              optimizer='rmsprop',\n","              metrics=metrics.SparseCategoricalAccuracy('accuracy'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"d0CApiMdunOF"},"outputs":[],"source":["#@title CNN model\n","\n","nClasses = 2\n","\n","embedding_layer = Embedding(len(word_index) + 1,\n","                            EMBEDDING_DIM,\n","                            weights=[embedding_matrix],\n","                            input_length=MAX_SEQUENCE_LENGTH,\n","                            trainable=True)\n","\n","sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH, ))\n","embedded_sequences = embedding_layer(sequence_input)\n","\n","x = BatchNormalization()(embedded_sequences)\n","x = Conv1D(256, 5, activation='relu')(x)\n","x = MaxPooling1D(5)(x)\n","x = Conv1D(256, 5, activation='relu')(x)\n","x = MaxPooling1D(5)(x)\n","x = Conv1D(256, 5, activation='relu')(x)\n","x = MaxPooling1D(5)(x)\n","x = Flatten()(x)\n","x = Dense(256, activation='relu')(x)\n","preds = Dense(nClasses, activation='softmax')(x)\n","model = Model(sequence_input, preds)\n","\n","optimizer = Adam(learning_rate=5e-4, beta_1=0.9, beta_2=0.999, \n","                epsilon=1e-07, amsgrad=False)\n","\n","model.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n","                optimizer='rmsprop',\n","                metrics=metrics.SparseCategoricalAccuracy('accuracy'))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"thD1j7HVm148"},"outputs":[],"source":["#@title Training Model\n","\n","model_type = 'CNN' #@param {type: \"string\"}\n","BATCH_SIZE = 64 #@param {type:\"number\"}\n","EPOCHS =   4 #@param {type:\"number\"}\n","\n","if model_type == \"RNN\":\n","    model.fit([train_data[:,:-1], train_data[:,-1]], y_train.astype(float),\n","                validation_split=0.2,\n","                epochs=EPOCHS,\n","                batch_size=BATCH_SIZE)\n","    model.save(('/content/drive/MyDrive/Projects/datacamp/Text/rnnft1_model.h5'))\n","elif model_type == \"CNN\":\n","    model.fit(train_data, y_train.astype(float),\n","            validation_split=0.2, \n","            epochs=EPOCHS,\n","            batch_size=BATCH_SIZE)\n","    model.save(('/content/drive/MyDrive/Projects/datacamp/Text/cnnft_model.h5'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"fbIH_NVKKWJ-"},"outputs":[],"source":["#@title Test FastText models\n","\n","\n","model_type = 'CNN' #@param {type: \"string\"}\n","if model_type == 'RNN':\n","    model = load_model('/content/drive/MyDrive/Projects/datacamp/Text/rnnft_model.h5')\n","elif model_type == 'CNN':\n","    model = load_model('/content/drive/MyDrive/Projects/datacamp/Text/cnnft_model.h5')\n","\n","test_result = model.evaluate(test_data)\n","predictions = model.predict(test_data)\n","label = np.where(predictions > (0.5 + threshold), 1, arr)\n","label = np.where(arr < (0.5 - threshold), 0, arr)\n","label = np.where((arr <= (0.5 + threshold)) &(arr >= (0.5 - threshold)),\n","                 2, arr)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOifwA2wLMUFbfk2/HTW6ho","collapsed_sections":[],"name":"text_classification.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.1 (main, Dec 23 2022, 09:25:23) [Clang 14.0.0 (clang-1400.0.29.202)]"},"vscode":{"interpreter":{"hash":"5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"}}},"nbformat":4,"nbformat_minor":0}

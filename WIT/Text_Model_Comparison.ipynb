{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WIT Toxicity Text Model Comparison",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SalarNouri/Advanced-Deep-Learning/blob/main/WIT/Text_Model_Comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiNxsd4_q9wq"
      },
      "source": [
        "This notebook shows use of the [What-If Tool](https://pair-code.github.io/what-if-tool) to compare two text models that determine sentence toxicity, one of which has had some debiasing performed during training.\n",
        "\n",
        "This notebook loads two pretrained toxicity models from [ConversationAI](https://github.com/conversationai/unintended-ml-bias-analysis) and compares them on the [wikipedia comments dataset](https://figshare.com/articles/Wikipedia_Talk_Labels_Toxicity/4563973).\n",
        "\n",
        "This notebook also shows how the What-If Tool can be used on non-TensorFlow models. In this case, these models are keras models that do not use tensorflow Examples as an input format. These models can be analyzed in the What-If Tool by supplying a custom prediction function to WitWidget.\n",
        "\n",
        "It also shows use of a user-provided custom distance function (for counterfactual analysis and datapoint similarity visualizations). The [tf.Hub Universal Sentence Encoder](https://tfhub.dev/google/universal-sentence-encoder/2) is used to compute the similarity of the input text comments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqB2tjOMETmr"
      },
      "source": [
        "#@title Install the What-If Tool widget if running in colab {display-mode: \"form\"}\n",
        "\n",
        "# If running in colab then pip install, otherwise no need.\n",
        "try:\n",
        "  import google.colab\n",
        "  !pip install --upgrade tensorflow>=2.0.0 witwidget \n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBOHfrOP7Iy5"
      },
      "source": [
        "#@title Download the pretrained keras model files\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_wiki_tox_v3_model.h5 -o ./cnn_wiki_tox_v3_model.h5\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_wiki_tox_v3_hparams.h5 -o ./cnn_wiki_tox_v3_hparams.h5\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_wiki_tox_v3_tokenizer.pkl -o ./cnn_wiki_tox_v3_tokenizer.pkl\n",
        "\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_debias_tox_v3_model.h5 -o ./cnn_debias_tox_v3_model.h5\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_debias_tox_v3_hparams.h5 -o ./cnn_debias_tox_v3_hparams.h5\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_debias_tox_v3_tokenizer.pkl -o ./cnn_debias_tox_v3_tokenizer.pkl\n",
        "\n",
        "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/wiki_test.csv -o ./wiki_test.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZR3i6UZlZ96"
      },
      "source": [
        "#@title Load the keras models\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from six.moves import cPickle as pkl\n",
        "\n",
        "def pkl_load(f):\n",
        "  return pkl.load(f) if sys.version_info < (3, 0) else pkl.load(\n",
        "    f, encoding='latin1')\n",
        "\n",
        "model1 = tf.keras.models.load_model('cnn_wiki_tox_v3_model.h5')\n",
        "with open('cnn_wiki_tox_v3_tokenizer.pkl', 'rb') as f:\n",
        "  tokenizer1 = pkl_load(f)\n",
        "tokenizer1.oov_token = None # quick fix for version issues\n",
        "\n",
        "model2 = tf.keras.models.load_model('cnn_debias_tox_v3_model.h5')\n",
        "with open('cnn_debias_tox_v3_tokenizer.pkl', 'rb') as f:\n",
        "  tokenizer2 = pkl_load(f)\n",
        "tokenizer2.oov_token = None # quick fix for version issues"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nStoYhqT80WH"
      },
      "source": [
        "#@title Define custom prediction functions so that WIT infers using keras models\n",
        "import tensorflow as tf\n",
        "\n",
        "# Set up model helper functions:\n",
        "PADDING_LEN = 250\n",
        "\n",
        "# Convert list of tf.Examples to list of comment strings.\n",
        "def examples_to_strings(examples):\n",
        "  texts = [ex.features.feature['comment'].bytes_list.value[0] for ex in examples]\n",
        "  if sys.version_info >= (3, 0):\n",
        "    texts = [t.decode('utf-8') for t in texts]\n",
        "  return texts\n",
        "\n",
        "# Get raw string out of tf.Example and prepare it for keras model input\n",
        "def examples_to_model_in(examples, tokenizer):\n",
        "  texts = examples_to_strings(examples)\n",
        "  # Tokenize string into fixed length sequence of integer based on tokenizer \n",
        "  # and model padding\n",
        "  text_sequences = tokenizer.texts_to_sequences(texts)\n",
        "  model_ins = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      text_sequences, maxlen=PADDING_LEN)\n",
        "  return model_ins\n",
        "\n",
        "# WIT predict functions:\n",
        "def custom_predict_1(examples_to_infer):\n",
        "  model_ins = examples_to_model_in(examples_to_infer, tokenizer1)\n",
        "  preds = model1.predict(model_ins)\n",
        "  return preds\n",
        "\n",
        "def custom_predict_2(examples_to_infer):\n",
        "  model_ins = examples_to_model_in(examples_to_infer, tokenizer2)\n",
        "  preds = model2.predict(model_ins)\n",
        "  return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXaUORW0DVhg"
      },
      "source": [
        "#@title Define helper functions for dataset conversion from csv to tf.Examples\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Converts a dataframe into a list of tf.Example protos.\n",
        "def df_to_examples(df, columns=None):\n",
        "  examples = []\n",
        "  if columns == None:\n",
        "    columns = df.columns.values.tolist()\n",
        "  for index, row in df.iterrows():\n",
        "    example = tf.train.Example()\n",
        "    for col in columns:\n",
        "      if df[col].dtype is np.dtype(np.int64):\n",
        "        example.features.feature[col].int64_list.value.append(int(row[col]))\n",
        "      elif df[col].dtype is np.dtype(np.float64):\n",
        "        example.features.feature[col].float_list.value.append(row[col])\n",
        "      elif row[col] == row[col]:\n",
        "        example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n",
        "    examples.append(example)\n",
        "  return examples\n",
        "\n",
        "# Converts a dataframe column into a column of 0's and 1's based on the provided test.\n",
        "# Used to force label columns to be numeric for binary classification using a TF estimator.\n",
        "def make_label_column_numeric(df, label_column, test):\n",
        "  df[label_column] = np.where(test(df[label_column]), 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu398ARdeuxe"
      },
      "source": [
        "#@title Read the dataset from CSV and process it for model {display-mode: \"form\"}\n",
        "import pandas as pd\n",
        "\n",
        "# Set the path to the CSV containing the dataset to train on.\n",
        "csv_path = 'wiki_test.csv'\n",
        "\n",
        "# Set the column names for the columns in the CSV. If the CSV's first line is a header line containing\n",
        "# the column names, then set this to None.\n",
        "csv_columns = None\n",
        "\n",
        "# Read the dataset from the provided CSV and print out information about it.\n",
        "df = pd.read_csv(csv_path, names=csv_columns, skipinitialspace=True)\n",
        "df = df[['is_toxic', 'comment']]\n",
        "\n",
        "# Remove non ascii characters\n",
        "comments = df['comment'].values\n",
        "proc_comments = []\n",
        "comment_lengths = []\n",
        "for c in comments:\n",
        "  try:\n",
        "    if sys.version_info >= (3, 0):\n",
        "      c = bytes(c, 'utf-8')\n",
        "    c = c.decode('unicode_escape')\n",
        "    if sys.version_info < (3, 0):\n",
        "      c = c.encode('ascii', 'ignore')\n",
        "    proc_comment = c.strip()\n",
        "  except:\n",
        "    proc_comment = ''\n",
        "  proc_comments.append(proc_comment)\n",
        "  comment_lengths.append(len(proc_comment.split()))\n",
        "\n",
        "df = df.assign(comment=proc_comments)\n",
        "df['comment length'] = comment_lengths\n",
        "\n",
        "label_column = 'is_toxic'\n",
        "make_label_column_numeric(df, label_column, lambda val: val)\n",
        "\n",
        "examples = df_to_examples(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVaMyc45HWwD"
      },
      "source": [
        "#@title Define a custom distance function for comparing datapoints (uses tf.Hub) {display-mode: \"form\"}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "# For this use-case, we set distance between datapoints to be cosine distance\n",
        "# between unit-normalized embeddings of each datapoint from the tf.Hub\n",
        "# Universal Sentence Encoder.\n",
        "def universal_sentence_encoder_distance(input_example, examples_to_compare, _):\n",
        "  # Extract comment strings\n",
        "  input_sentence = examples_to_strings([input_example])[0]\n",
        "  sentences = examples_to_strings(examples_to_compare)\n",
        "\n",
        "  # Normalize all embeddings for cosine distance operation\n",
        "  input_emb = tf.squeeze(tf.nn.l2_normalize(embed([input_sentence]), axis=1))\n",
        "  sentences_emb = tf.nn.l2_normalize(embed(sentences), axis=1)\n",
        "\n",
        "  # Tile the input example for easy comparison to all examples\n",
        "  multiply = tf.constant([len(examples_to_compare)])\n",
        "  input_matrix = tf.reshape(tf.tile(input_emb, multiply),\n",
        "                            [multiply[0], tf.shape(input_emb)[0]])\n",
        "  \n",
        "  # Compute cosine distance from input example to all examples.\n",
        "  cosine_distance = tf.keras.losses.CosineSimilarity(\n",
        "      axis=1, reduction=tf.losses.Reduction.NONE)\n",
        "  distances = cosine_distance(sentences_emb, input_matrix)\n",
        "  results = tf.squeeze(distances)\n",
        "  return results.numpy().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwiWGrLlSWGh"
      },
      "source": [
        "#@title Invoke What-If Tool for the data and two models (Note that this step may take a while due to prediction speed of the toxicity model){display-mode: \"form\"}\n",
        "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
        "num_datapoints = 1000  #@param {type: \"number\"}\n",
        "tool_height_in_px = 720  #@param {type: \"number\"}\n",
        "\n",
        "# Setup the tool with the test examples and the trained classifier\n",
        "config_builder = WitConfigBuilder(examples[:num_datapoints]).set_custom_predict_fn(\n",
        "  custom_predict_1).set_compare_custom_predict_fn(custom_predict_2).set_custom_distance_fn(\n",
        "      universal_sentence_encoder_distance)\n",
        "\n",
        "wv = WitWidget(config_builder, height=tool_height_in_px)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}